@misc{2048,
    author={Cirulli, G},
    title={2048, available from ⟨http://gabrielecirulli.github.io/2048/⟩},
    year={2014}
}

@phdthesis{solving,
    title = "Searching for solutions in games and artificial intelligence",
    author = "L.V. Allis",
    year = "1994",
    month = jan,
    day = "1",
    doi = "10.26481/dis.19940923la",
    language = "English",
    isbn = "9090074880",
    publisher = "Rijksuniversiteit Limburg",
    school = "Maastricht University",
}

@inproceedings{3x3_2048,
    author	 = "山下修平 and 金子知適 and 中屋敷太一",
    title	 = "3×3盤面の2048の完全解析と強化学習の研究",
    booktitle	 = "ゲームプログラミングワークショップ2022論文集",
    year 	 = "2022",
    volume	 = "2022",
    number  = "",
    series  = "",
    pages	 = "1--8",
    month	 = "nov"
}

@inproceedings{4x3_2048,
   author	 = "山下修平 and 金子知適",
   title	 = "4×3盤面の2048の完全解析",
   booktitle	 = "ゲームプログラミングワークショップ2023論文集",
   year 	 = "2023",
   volume	 = "2023",
   number	 = "",
   pages	 = "1--5",
   month	 = "nov"
}

@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018}
}

@article{deepRL,
  author       = {Vincent Fran{\c{c}}ois{-}Lavet and
                  Peter Henderson and
                  Riashat Islam and
                  Marc G. Bellemare and
                  Joelle Pineau},
  title        = {An Introduction to Deep Reinforcement Learning},
  journal      = {CoRR},
  volume       = {abs/1811.12560},
  year         = {2018},
  url          = {http://arxiv.org/abs/1811.12560},
  eprinttype    = {arXiv},
  eprint       = {1811.12560},
  timestamp    = {Mon, 03 Dec 2018 07:50:28 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1811-12560.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{Szubert,
  author={Szubert, Marcin and Jaśkowski, Wojciech},
  booktitle={2014 IEEE CIG}, 
  title={Temporal difference learning of N-tuple networks for the game 2048}, 
  year={2014},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/CIG.2014.6932907}}

@article{KiminoriMatsuzaki2021,
  title={Developing Value Networks for Game 2048 with Reinforcement Learning},
  author={Kiminori Matsuzaki},
  journal={Journal of Information Processing},
  volume={29},
  number={ },
  pages={336-346},
  year={2021},
  doi={10.2197/ipsjjip.29.336}
}

@misc{PPO,
    Author = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
    Title = {Proximal Policy Optimization Algorithms},
    Year = {2017},
    note = {https://arxiv.org/abs/1707.06347}
}

@misc{GAE,
      title={High-Dimensional Continuous Control Using Generalized Advantage Estimation}, 
      author={John Schulman and Philipp Moritz and Sergey Levine and Michael Jordan and Pieter Abbeel},
      year={2018},
      eprint={1506.02438},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{guei,
  title={An Early Attempt at Applying Deep Reinforcement Learning to the Game 2048},
  author = {Hung Guei and Tinghan Wei and Ching-Bo Huang and I-Chen Wu},
  year = {2016},
  journal={Workshop on Neural Networks in Games}
}

@article{DQN,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Andrei A. Rusu and
               Joel Veness and
               Marc G. Bellemare and
               Alex Graves and
               Martin A. Riedmiller and
               Andreas Fidjeland and
               Georg Ostrovski and
               Stig Petersen and
               Charles Beattie and
               Amir Sadik and
               Ioannis Antonoglou and
               Helen King and
               Dharshan Kumaran and
               Daan Wierstra and
               Shane Legg and
               Demis Hassabis},
  title     = {Human-level control through deep reinforcement learning},
  journal   = {Nat.},
  volume    = {518},
  number    = {7540},
  pages     = {529--533},
  year      = {2015},
  url       = {https://doi.org/10.1038/nature14236},
  doi       = {10.1038/nature14236},
  timestamp = {Mon, 08 Jun 2020 22:21:32 +0200},
  biburl    = {https://dblp.org/rec/journals/nature/MnihKSRVBGRFOPB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{AlphaZero,
  author = {David Silver  and Thomas Hubert  and Julian Schrittwieser  and Ioannis Antonoglou  and Matthew Lai  and Arthur Guez  and Marc Lanctot  and Laurent Sifre  and Dharshan Kumaran  and Thore Graepel  and Timothy Lillicrap  and Karen Simonyan  and Demis Hassabis },
  title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  journal = {Science},
  volume = {362},
  number = {6419},
  pages = {1140-1144},
  year = {2018},
  doi = {10.1126/science.aar6404}, 

  URL = {https://www.science.org/doi/abs/10.1126/science.aar6404},
  eprint = {https://www.science.org/doi/pdf/10.1126/science.aar6404}, 
  abstract = { Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system. Science, this issue p. 1140; see also pp. 1087 and 1118 AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each. The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go. }
}

@Article{MuZero,
author={Schrittwieser, Julian
and Antonoglou, Ioannis
and Hubert, Thomas
and Simonyan, Karen
and Sifre, Laurent
and Schmitt, Simon
and Guez, Arthur
and Lockhart, Edward
and Hassabis, Demis
and Graepel, Thore
and Lillicrap, Timothy
and Silver, David},
title={Mastering Atari, Go, chess and shogi by planning with a learned model},
journal={Nature},
year={2020},
month={Dec},
day={01},
volume={588},
number={7839},
pages={604-609},
abstract={Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3---the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4---the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi---canonical environments for high-performance planning---the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
issn={1476-4687},
doi={10.1038/s41586-020-03051-4},
url={https://doi.org/10.1038/s41586-020-03051-4}
}

@inproceedings{StochasticMuZero,
title={Planning in Stochastic Environments with a Learned Model},
author={Ioannis Antonoglou and Julian Schrittwieser and Sherjil Ozair and Thomas K Hubert and David Silver},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=X6D9bAHhBQ1}
}

@Book{DeepLearning,
  Title                    = {Deep Learning},
  Author                   = {Ian J. Goodfellow and Yoshua Bengio and Aaron Courville},
  Publisher                = {MIT Press},
  Year                     = {2016},

  Address                  = {Cambridge, MA, USA},
  Note                     = {\url{http://www.deeplearningbook.org}}
}