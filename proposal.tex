\chapter{提案手法}
\label{chap:proposal}
\ref{sec:rlto2048}節で述べたように， 2048を対象とした強化学習の研究は数多くなされてきた．
もしゲームが完全に解かれていれば， 強化学習手法の良し悪しを定量的に評価することができる．
一方で2048はゲーム木の大きさから， 完全解析を実行することは計算資源の観点から困難である．
そこで本研究では2048のミニゲームの完全解析を行うことを提案する．
さらに解析したミニゲームをベンチマークとして， 2048の強化学習手法について詳細に検討する．

\input{solving.tex}

\section{2048のミニゲームの完全解析と強化学習}
\ref{chap:solving}節では強化学習のベンチマークとしての2048のミニゲームの提案， およびその完全解析を行った．
これらのミニゲームをベンチマークとすることで， 2048の強化学習手法の定量的評価を行うことができる．
また$4\times4$盤面の
これを題材として本研究では， ~\ref{subsec: stochastic_muzero}節で述べたStochastic MuZero~\cite{StochasticMuZero}について詳細に研究する．
ただしStochastic MuZeroのように， 環境のダイナミクスモデルを学習するには多くの計算資源を要する．
そのため本稿では， 環境のダイナミクスは既知として， 方策・価値ネットワークの訓練のみを行う2048-AlphaZeroを考える．

\subsection{$3\times3$盤面の2048における強化学習の研究}
本節では$3\times3$盤面の2048を題材として， 2048-AlphaZeroの評価を行う．
2048-AlphaZeroは訓練時と， 訓練後の性能の評価時の両方でMCTSによるプランニングを行う．
\ref{subsubsec:mcts}節で述べたように， MCTSはシミュレーションという単位で実行される．
シミュレーションの回数が大きいほど， 探索は深く正確になる．
以降では訓練時のシミュレーション回数が$n$回， 評価時のシミュレーション回数が$m$回のエージェントを$\text{train-}n,\text{eval-}m$のように表す．
また評価時には$1,000$回のゲームの平均得点を用いる．

\subsubsection{$3\times3$盤面の2048における2048-AlphaZeroの学習}
まず一様ランダムに学習データをサンプルするExperience Replay~(以降Random Experience Replayと表記する)~を用いる手法と， Stochastic MuZeroと同様のpriorityに従ってサンプルするPrioritized Experience Replayを用いる手法の$2$つを比較する．
ここでは$300\times{10}^6$回の行動分の学習データを用いて訓練した．
図~\ref{fig:mini2048_alphazero}にそれぞれのエージェント$(\text{いずれもtrain-}100,\text{eval-}100)$の学習の様子を示す．
図中のoptimal value~(最適価値)~とは~\ref{sec:solving}節の完全解析によって得た， $3\times3$盤面の2048の得点の理論値である．
平均得点の最大値は， Random Experience Replayを用いた場合が$5,216.50$点， Prioritized Experience Replayを用いた場合が$5,354.10$点であった．
いずれも学習に従って最適価値に近い得点を獲得するようになるが， Prioritized Experience Replayを使用した方が僅かに点数が伸びる様子が見て取れる．

\subsubsection{MCTSによるプランニングと得点の関係の検証}
次にMCTSによるプランニングが， エージェントの強さにどのように寄与しているかを調べる．
そのために学習時と評価時のMCTSのシミュレーション回数をそれぞれ増減させたときに， エージェントの得点がどう変化するかを実験する．
シミュレーション回数が$10$回， $20$回， $50$回， $100$回で訓練したエージェントそれぞれについて， $0 \sim 100$回のシミュレーションで評価した結果を図~\ref{fig:mini2048_alphazero_simulations}に示す．
ただしシミュレーション$0$回のMCTSとは， 状態$s$において方策ネットワークが示す$\pi(\cdot|s)$が最も高い行動を常に選択するものとする．
またいずれの条件においても学習時にはPrioritized Experience Replayを用いる．
図~\ref{fig:mini2048_alphazero_simulations}からtrain-$50$とtrain-$100$は， 最適価値付近の得点を獲得できている．
さらにこれらは評価時にはシミュレーション回数を少なくしても， 得点への影響はあまりないことがわかる．
ニューラルネットワークが十分に訓練されていれば， MCTSによるプランニングを行わなくてもエージェントは正確な行動を取ることができるといえる．
一方train-$10$とtrain-$20$は最適価値を大きく下回る結果となった．

特に訓練時により多くのプランニングを行ったtrain-$20$が， train-$10$よりも得点が低い結果が得られたのは興味深い．
train-$100$のエージェントは， 評価時のシミュレーション回数を$100$回から減らしても得点はあまり減少しない．
一方で
\begin{figure}
    \begin{subfigure}[T]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/alphazero_3x3.pdf}
        \caption{$3\times3$盤面の2048-alphazero}
        \label{fig:mini2048_alphazero}
    \end{subfigure}
    \begin{subfigure}[T]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/3x3_2048_alphazero_simulations.pdf}
        \caption{$3\times3$盤面の2048-alphazero}
        \label{fig:mini2048_alphazero_simulations}
    \end{subfigure}
\end{figure}

Terauchiら~\cite{Min2048_matsuzaki}は$3\times3$盤面の2048の完全解析結果を題材に， TD-afterstate学習でN-tupleネットワークを訓練したプレイヤを様々な点から評価した．
Terauchiらはパラメータ数が$7.1\times{10}^6$のN-tupleネットワークを$500 \times {10}^6$回の行動分の学習データを用いて訓練したが， 平均$4905.27$点にとどまったと報告している．
一方で本研究で実験した2048-AlphaZeroは， パラメータ数が$568,717$の状態を評価する方策・価値ニューラルネットワークと， パラメータ数$529,417$のafterstateを評価する価値ニューラルネットワークを持つ．
$\text{train-}100,\text{eval-}100$のエージェントは平均$5,354.10$点を達成し， 評価時にMCTSを行わないエージェント$(\text{train-}100,\text{eval-}0)$は平均$5,125.01$点を達成した．
よってTerauchiらのN-tupleネットワーク+TD afterstate学習と比べて2048-AlphaZeroは， 約$7$分の$1$のパラメータ数， $5$分の$3$の学習データを用いて平均得点を上回ることができた．
これらの結果を表~\ref{table:terauchi}にまとめる．
\begin{table}[t]
    \centering
    \begin{tabular}{lrrrr}
        \hline \hline
        手法 & 平均得点 & 訓練時のプランニング & 評価時のプランニング & パラメータ数\\ \hline
        TD afterstate~\cite{Min2048_matsuzaki} & $4,905.27$ & なし & なし & $7,100,000$\\
        2048-AlphaZero & $5,125.01$ & $100$シミュレーション & なし & $1,098,134$\\
        2048-AlphaZero & $5,354.10$ & $100$シミュレーション & $100$シミュレーション & $1,098,134$ \\
        \hline
    \end{tabular}
    \caption{N-tuple+TD afterstate学習と2048-AlphaZeroの比較}
    \label{table:terauchi}
\end{table}

この要因としては以下が考えられる．
\begin{itemize}
    \item ニューラルネットワークの関数近似手法としての優れた汎化性能
    \item 訓練時のMCTSによる良質な学習データの獲得
\end{itemize}

\subsubsection{完全解析結果を活用した学習が難しい盤面の調査}
ここまでは$3\times3$盤面の2048の最適価値とエージェントの獲得する得点を比較してきた．
本節では完全解析結果のデータベースをさらに活用して， 学習が難しい盤面を調査する．
本研究では盤面上の空きマスの数に着目した．
一般に空きマスの数が少ない盤面は， ゲームオーバーになる可能性が高く正しい行動をとるのが難しい．
そのため学習時においても正確な価値を推定するのが難しいと考えられる．
そこで図~\ref{fig:empty_tiles_accuracy}に盤面上の空きマスの数と， それらの盤面におけるエージェントが示す手と最善手との一致率の関係を示す．
また図~\ref{fig:empty_tiles_abs_error}に盤面上の空きマスの数と， それらの盤面におけるニューラルネットワークの評価値と最適価値の絶対誤差の関係を示す．
実験ではいずれにおいてもPrioritized Experience Replayで訓練したtrain-$100$， eval-$100$のエージェントを用いた．
ここでは$1,000$ゲームをプレイする中で現れたすべての盤面を対象とした．
2048のルールでは常に盤面上に少なくとも$2$つの数字タイルが存在するため， $3\times3$盤面の2048では空きマスの数の最大値は$7$である．

空きマスの数が$7$の場合を除いて， 空きマスが増えるほど最善手との一致率は
\begin{figure}
    \begin{subfigure}[T]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/empty_tiles_accuracy.pdf}
        \caption{盤面上の空きマスの数と最善手との一致率}
        \label{fig:empty_tiles_accuracy}
    \end{subfigure}
    \begin{subfigure}[T]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/empty_tiles_abs_error.pdf}
        \caption{最適価値とニューラルネットワークが評価する推定値の絶対誤差}
        \label{fig:empty_tiles_abs_error}
    \end{subfigure}
\end{figure}

\subsection{$4\times3$盤面の2048と2048-AlphaZero}
$3\times3$盤面の2048は， 強化学習手法の一手一手の詳細な分析を行うのに適しているといえる．
一方で図~\ref{fig:alphazero_3x3}の結果が示すように， プレイヤの差が現れにくい．
そのため本節ではより難易度の高い$4\times3$盤面の2048において， 2048-AlphaZeroを評価する．