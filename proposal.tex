\chapter{提案手法}
\label{chap:proposal}
\ref{sec:rlto2048}節で述べたように， 2048を対象とした強化学習の研究は数多くなされてきた．
もしゲームが完全に解かれていれば， 強化学習手法の良し悪しを定量的に評価することができる．
一方で2048はゲーム木の大きさから， 完全解析を実行することは計算資源の観点から困難である．
そこで本研究では2048のミニゲームの完全解析を行うことを提案する．
さらに解析したミニゲームをベンチマークとして， 2048の強化学習手法について詳細に検討する．

\input{solving.tex}

\section{2048のミニゲームの完全解析と強化学習}
\ref{chap:solving}節では強化学習のベンチマークとしての2048のミニゲームの提案， およびその完全解析を行った．
これを題材として本研究では， ~\ref{subsec: stochastic_muzero}節で述べたStochastic MuZero~\cite{StochasticMuZero}について詳細に研究する．
ただしStochastic MuZeroのように， 環境のダイナミクスモデルを学習するには多くの計算資源を要する．
そのため本稿では， 環境のダイナミクスは既知として， 方策・価値ネットワークの訓練のみを行う2048-AlphaZeroを考える．

\subsection{$3\times3$盤面の2048と2048-AlphaZero}
本節では$3\times3$盤面の2048を題材として， 2048-AlphaZeroの評価を行う．
2048-AlphaZeroは訓練時と， 訓練後の性能の評価時の両方でMCTSによるプランニングを行う．
\ref{subsubsec:mcts}節で述べたように， MCTSはシミュレーションという単位で実行される．
シミュレーションの回数が大きいほど， 探索は深く正確になる．
以降では訓練時のシミュレーション回数が$n$回， 評価時のシミュレーション回数が$m$回のエージェントを$(\text{train-}n,\text{eval-}m)$と表す．
また評価時には$1,000$回のゲームの平均得点を用いる．

まず一様ランダムに学習データをサンプルするExperience Replay~(以降Random Experience Replayと表記する)~を用いる手法と， Stochastic MuZeroと同様のpriorityに従ってサンプルするPrioritized Experience Replayを用いる手法の$2$つを比較する．
図~\ref{fig:mini2048_alphazero}にそれぞれのエージェント$(\text{train-}100,\text{eval-}100)$の学習の様子を示す．
図中のoptimal valueとは~\ref{sec:solving}節の完全解析によって得た， $3\times3$盤面の2048の理論値である．
平均得点の最大値は， Random Experience Replayを用いた場合が$5,216.50$点， Prioritized Experience Replayを用いた場合が$5,354.10$点であった．
いずれも学習に従って最適価値に近い得点を獲得するようになるが， Prioritized Experience Replayを使用した方が僅かに点数が伸びる様子が見て取れる．

次にMCTSによるプランニングが， 学習および評価時のエージェントの強さにどのように寄与しているかを調べる．
そのために学習時と評価時のMCTSのシミュレーション回数を増減させたときに， エージェントの得点がどう変化するかを実験する．
訓練時のシミュレーション回数が$10$回， $20$回， $50$回， $100$回のエージェントそれぞれについて， $0 \sim 100$回のシミュレーションで評価した結果を図~\ref{fig:mini2048_alphazero_simulations}に示す．
ただしシミュレーション$0$回のMCTSとは， 状態$s$において方策ネットワークが示す$\pi(\cdot|s)$が最も高い行動を常に選択するものとする．
\begin{figure}
    \begin{subfigure}[T]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/alphazero_3x3.pdf}
        \caption{$3\times3$盤面の2048-alphazero}
        \label{fig:mini2048_alphazero}
    \end{subfigure}
    \begin{subfigure}[T]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/3x3_2048_alphazero_simulations.pdf}
        \caption{$3\times3$盤面の2048-alphazero}
        \label{fig:mini2048_alphazero_simulations}
    \end{subfigure}
\end{figure}

Terauchiら~\cite{Min2048_matsuzaki}は$3\times3$盤面の2048の完全解析結果を題材に， TD-afterstate学習でN-tupleネットワークを訓練したプレイヤを様々な点から評価した．
Terauchiらはパラメータ数が$7.1\times{10}^6$のN-tupleネットワークを訓練したが， 平均$4905.27$点にとどまったと報告している．
一方で本研究で実験した2048-AlphaZeroは， パラメータ数が$568,717$の状態を評価する方策・価値ニューラルネットワークと， パラメータ数$529,417$のafterstateを評価する価値ニューラルネットワークを持つ．
よってTerauchiらのN-tupleネットワークと比べて約$7$分の$1$のパラメータ数で， 平均得点を大幅に上回ることができたといえる．

この要因としては以下が考えられる．
\begin{itemize}
    \item ニューラルネットワークの関数近似手法としての優れた汎化性能
    \item 
\end{itemize}

\subsection{$4\times3$盤面の2048と2048-AlphaZero}
$3\times3$盤面の2048は， 強化学習手法の一手一手の詳細な分析を行うのに適しているといえる．
一方で図~\ref{fig:alphazero_3x3}の結果が示すように， プレイヤの差が現れにくい．
そのため本節ではより難易度の高い$4\times3$盤面の2048において， 2048-AlphaZeroを評価する．