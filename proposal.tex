\chapter{提案手法}
\label{chap:proposal}
\ref{sec:rlto2048}節で述べたように， 2048を対象とした強化学習の研究は数多くなされてきた．
もしゲームが完全に解かれていれば， 強化学習手法の良し悪しを定量的に評価することができる．
一方で2048はゲーム木の大きさから， 完全解析を実行することは計算資源の観点から困難である．
そこで本研究では2048のミニゲームの完全解析を行うことを提案する．
さらに解析したミニゲームをベンチマークとして， 2048の強化学習手法について詳細に検討する．

\input{solving.tex}

\section{2048のミニゲームの完全解析と強化学習}
\ref{chap:solving}節では強化学習のベンチマークとしての2048のミニゲームの提案， およびその完全解析を行った．
これを題材として本研究では， ~\ref{subsec: stochastic_muzero}節で述べたStochastic MuZero~\cite{StochasticMuZero}について詳細に研究する．
ただしStochastic MuZeroのように， 環境のダイナミクスモデルを学習するには多くの計算資源を要する．
そのため本稿では， 環境のダイナミクスは既知として， 方策・価値ネットワークの訓練のみを行う2048-AlphaZeroを考える．

\subsection{$3\times3$盤面の2048と2048-AlphaZero}
本節では$3\times3$盤面の2048を題材として， 2048-AlphaZeroの評価を行う．

図~\ref{fig:alphazero_3x3}にシミュレーション数$100$回のMCTSによるデータで訓練した， 2048-AlphaZeroの学習の様子を示す．
なおプレイヤの評価時においてもシミュレーション数$100$回のMCTSによって行動を決定する．
一様ランダムに学習データをサンプルするExperience Replay~(以降Random Experience Replayと表記する)~を用いる手法と， Stochastic MuZeroと同様の優先度に従ってサンプルするPrioritized Experience Replayを用いる手法の$2$つを実験した．
平均得点の最大値は， Experience Replayを用いた場合が$5,216.50$点， Prioritized Experience Replayを用いた場合が$5,354.10$点であった．
いずれも学習に従って最適価値に近い得点を獲得するようになるが， Prioritized Experience Replayを使用した方が僅かに点数が伸びる様子が見て取れる．
\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth{}]{figures/alphazero_3x3.pdf}
    \caption{$3\times3$盤面の2048-alphazero}
    \label{fig:alphazero_3x3}
\end{figure}

Terauchiら~\cite{Min2048_matsuzaki}は$3\times3$盤面の2048の完全解析結果を題材に， TD-afterstate学習でN-tupleネットワークを訓練したプレイヤを様々な点から評価した．
Terauchiらはパラメータ数が$7.1\times{10}^6$のN-tupleネットワークを訓練し， 平均$4905.27$点を達成したと報告している．
本研究で実験した2048-AlphaZeroは， パラメータ数が$568,717$の状態を評価する方策・価値ニューラルネットワークと， パラメータ数$529,417$のafterstateを評価する価値ニューラルネットワークを持つ．
よってTerauchiらのN-tupleネットワークと比べて約$7$分の$1$のパラメータ数で， 平均得点を大幅に上回ることができたといえる．
