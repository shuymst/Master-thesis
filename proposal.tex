\chapter{提案手法}
\label{chap:proposal}
\ref{sec:rlto2048}節で述べたように， 2048を対象とした強化学習の研究は数多くなされてきた．
もしゲームが完全に解かれていれば， 強化学習手法の良し悪しを定量的に評価することができる．
一方で2048はゲーム木の大きさから， 完全解析を実行することは計算資源の観点から困難である．
そこで本研究では2048のミニゲームの完全解析を行うことを提案する．
さらに解析したミニゲームをベンチマークとして， 2048の強化学習手法について詳細に検討する．

\input{solving.tex}

\section{2048のミニゲームの完全解析と強化学習}
\ref{chap:solving}節では強化学習のベンチマークとしての2048のミニゲームの提案， およびその完全解析を行った．
これらのミニゲームをベンチマークとすることで， 2048の強化学習手法の定量的評価を行うことができる．
これを題材として本研究では， \ref{subsec: stochastic_muzero}節で述べた， 汎用的で2048においても現状最も有力で強化学習手法の$1$つであるStochastic MuZero~\cite{StochasticMuZero}について研究する．
ただしStochastic MuZeroのように， 環境のダイナミクスモデルを学習するには多くの計算資源を要する．
そのため本稿では， 環境のダイナミクスは既知として， 状態を評価する方策・価値ネットワーク， afterstateを評価する価値ネットワークの訓練のみを行う2048-AlphaZeroを考える．
2048-AlphaZeroの学習において， MCTSによるプランニングやニューラルネットワークの学習方法がどのように影響するのかを， 詳細に検証する．
本研究で実験する2048-AlphaZeroのニューラルネットワークは， Resnet~\cite{Resnet}のアーキテクチャによって実装された．
アーキテクチャの詳細は付録~\ref{subsec:nn_impl}に記述した．

\subsection{$3\times3$盤面の2048における強化学習の研究}
本節では$3\times3$盤面の2048を題材として， 2048-AlphaZeroの評価を行う．
$3\times3$盤面の2048ではいずれの実験においても， 図~\ref{fig:nn_arch}のResidual Blockは$5$個， num filtersは$64$のニューラルネットワークを用いた．

2048-AlphaZeroは訓練時と， 訓練後の性能の評価時の両方でMCTSによるプランニングを行う．
\ref{subsubsec:mcts}節で述べたように， MCTSはシミュレーションという単位で実行される．
シミュレーションの回数が大きいほど， 探索は深く正確になる．
以降では訓練時のシミュレーション回数が$n$回， 評価時のシミュレーション回数が$m$回のエージェントを$\text{train-}n,\text{eval-}m$のように表す．
また評価時には$1,000$回のゲームの平均得点を用いる．

\subsubsection{$3\times3$盤面の2048における2048-AlphaZeroの学習}
まず一様ランダムに学習データをサンプルするExperience Replay~(以降Random Experience Replayと表記する)~を用いる手法と， Stochastic MuZeroと同様に予測誤差をpriorityとして重み付けサンプルするPrioritized Experience Replayを用いる手法の$2$つを比較する．
ここでは$3\times{10}^8$回の行動分の学習データを用いて訓練した．
図~\ref{fig:mini2048_alphazero}にそれぞれのエージェント$(\text{いずれもtrain-}100,\text{eval-}100)$の学習の様子を示す．
図中のoptimal value~(最適価値)~とは~\ref{sec:solving}節の完全解析によって得た， $3\times3$盤面の2048の得点の理論値である．
平均得点の最大値は， Random Experience Replayを用いた場合が$5,216.50$点， Prioritized Experience Replayを用いた場合が$5,354.10$点であった．
いずれも学習に従って最適価値に近い得点を獲得するようになるが， Prioritized Experience Replayを使用した方が僅かに点数が伸びる様子が見て取れる．

\subsubsection{MCTSによるプランニングと得点の関係の検証}
次にMCTSによるプランニングが， エージェントの強さにどのように寄与しているかを調べる．
そのために学習時と評価時のMCTSのシミュレーション回数をそれぞれ増減させたときに， エージェントの得点がどう変化するかを実験する．
シミュレーション回数が$10$回， $20$回， $30$回， $50$回， $100$回で訓練したエージェントそれぞれについて， $0 \sim 100$回のシミュレーションで評価した結果を図~\ref{fig:mini2048_alphazero_simulations}に示す．
ただしシミュレーション$0$回のMCTSとは， 状態$s$において方策ネットワークが示す$\pi(\cdot|s)$が最も高い行動を常に選択するものとする．
またいずれの条件においても学習時にはPrioritized Experience Replayを用いる．

訓練時のシミュレーション回数が多いほど， 得点も大きくなることがわかる．
正確なプランニングを行うほど， より良質な学習データを生成することができていると考えられる．
ただしtrain-$50$とtrain-$100$には大きな差はなく， 一定回数以上のシミュレーションを行うことはあまり学習に寄与しないと考えられる．
またニューラルネットワークが十分に訓練されていれば， 評価時にMCTSのシミュレーション数を増減させても得点に大きな変化はない．
特にtrain-$100$は評価時にMCTSを行わなくても， $5,125.01$点と高い平均得点を獲得している．
\begin{figure}
    \begin{subfigure}[T]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/alphazero_3x3.pdf}
        \caption{$3\times3$盤面の2048-alphazero}
        \label{fig:mini2048_alphazero}
    \end{subfigure}
    \begin{subfigure}[T]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/3x3_2048_alphazero_simulations.pdf}
        \caption{MCTSのシミュレーション数の増減}
        \label{fig:mini2048_alphazero_simulations}
    \end{subfigure}
\end{figure}

Terauchiら~\cite{Min2048_matsuzaki}は$3\times3$盤面の2048の完全解析結果を題材に， TD-afterstate学習でN-tupleネットワークを訓練したプレイヤを様々な点から評価した．
Terauchiらはパラメータ数が$7.1\times{10}^6$のN-tupleネットワークを$5 \times {10}^8$回の行動分の学習データを用いて訓練したが， 平均$4905.27$点にとどまったと報告している．
一方で本研究で実験した2048-AlphaZeroは， パラメータ数が$568,717$の状態を評価する方策・価値ニューラルネットワークと， パラメータ数$529,417$のafterstateを評価する価値ニューラルネットワークを持つ．
$\text{train-}100,\text{eval-}100$のエージェントは平均$5,354.10$点を達成し， 評価時にMCTSを行わないエージェント$(\text{train-}100,\text{eval-}0)$は平均$5,125.01$点を達成した．
よってTerauchiらのN-tupleネットワーク+TD afterstate学習と比べて2048-AlphaZeroは， 約$7$分の$1$のパラメータ数， $5$分の$3$の学習データを用いて平均得点を上回ることができた．
これらの結果を表~\ref{table:terauchi}にまとめる．
\begin{table}[t]
    \centering
    \begin{tabular}{lrrrr}
        \hline \hline
        手法 & 平均得点 & 訓練時のプランニング & 評価時のプランニング & パラメータ数\\ \hline
        TD afterstate~\cite{Min2048_matsuzaki} & $4,905.27$ & なし & なし & $7,100,000$\\
        2048-AlphaZero & $5,125.01$ & $100$シミュレーション & なし & $1,098,134$\\
        2048-AlphaZero & $5,354.10$ & $100$シミュレーション & $100$シミュレーション & $1,098,134$ \\
        \hline
    \end{tabular}
    \caption{N-tuple+TD afterstate学習と2048-AlphaZeroの比較}
    \label{table:terauchi}
\end{table}

ここまでの実験を踏まえると， 2048-AlphaZeroが優れている要因として以下が考えられる．
\begin{itemize}
    \item ニューラルネットワークの関数近似手法としての優れた汎化性能
    \item 訓練時のMCTSによる良質な学習データの獲得
\end{itemize}
さらなる研究として， ニューラルネットワークを評価関数としてTD-afterstate学習を行う

\subsubsection{完全解析結果を活用した学習が難しい盤面の調査}
\label{subsubsec:db_analize}
ここまでは$3\times3$盤面の2048の最適価値とエージェントの獲得する得点を比較してきた．
本節では完全解析結果のデータベースをさらに活用して， 学習が難しい盤面を調査する．
本研究では盤面上の空きマスの数に着目した．
一般に空きマスの数が少ない盤面は， ゲームオーバーになる可能性が高く正しい行動をとるのが難しい．
そのため正確な価値を推定するように， ニューラルネットワークを学習させるのも難しいと考えられる．

そこで図~\ref{fig:empty_tiles_accuracy}に盤面上の空きマスの数と， それらの盤面におけるエージェントが示す手と最善手との一致率の関係を示す．
また図~\ref{fig:empty_tiles_abs_error}に盤面上の空きマスの数と， それらの盤面におけるエージェントが選んだ行動の価値と最善手の価値の絶対誤差~(ただしエージェントが最善手と異なる行動をとった回数で平均をとる)~の関係を示す．
いずれの実験においてもPrioritized Experience Replayで訓練したtrain-$100$， eval-$100$のエージェントを用いた．
ここでは$1,000$ゲームをプレイする中で現れたすべての状態を対象とした．
2048のルールでは常に盤面上に少なくとも$2$つの数字タイルが存在するため， $3\times3$盤面の2048では空きマスの数の最大値は$7$である．

図~\ref{fig:empty_tiles_accuracy}より空きマスの数が少ないと， エージェントの行動と最善手の一致率が低くなる傾向があることがわかる．
また図~\ref{fig:empty_tiles_abs_error}より空きマスの数が少ないほど， エージェントの行動価値と最善手の価値の絶対誤差は大きいことがわかる．
よって学習を効率化させるために， 空きマスが少ないデータを優先的に学習に使用するような， Prioritized Experience Replayを使用することが考えられる．
ただし$3\times3$盤面の2048においては， すでにエージェントの得点が最適価値に十分近づいているため， $4\times3$盤面の2048で実験することにする．
\begin{figure}
    \begin{subfigure}[T]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/empty_tiles_accuracy.pdf}
        \caption{盤面上の空きマスの数と最善手との一致率}
        \label{fig:empty_tiles_accuracy}
    \end{subfigure}
    \begin{subfigure}[T]{0.5\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{figures/empty_tiles_abs_error.pdf}
        \caption{最善手の価値とエージェントが選んだ行動の価値の絶対誤差}
        \label{fig:empty_tiles_abs_error}
    \end{subfigure}
\end{figure}

\subsection{$4\times3$盤面の2048と2048-AlphaZero}
$3\times3$盤面の2048は状態数が比較的少ないため， データベースが軽量でデータを参照しやすい利点がある．
そのため強化学習手法の一手一手の詳細な分析を行うのに適している環境といえる．
実際，\ref{subsubsec:db_analize}節ではデータベースを活用して， エージェントの一手ごとの行動について評価を行った．
一方で図~\ref{fig:mini2048_alphazero}が示すように， $3\times3$盤面の2048では学習したエージェントは得点が最適価値に十分近づいている．
そのため一定以上の性能を持つエージェントを比較するために適しているとは言えない．
よって本節では， より難易度の高い$4\times3$盤面の2048において， 2048-AlphaZeroを評価する．

$3\times3$盤面の実験ではResidual Blockが$5$個のニューラルネットワークのみを用いたが， $4\times3$盤面の実験では， これに加えてより大きな$10$個のニューラルネットワークでも実験を行う．
さらに~\ref{subsubsec:db_analize}節で述べた， 空きマスの少ない盤面データが優先的に学習に使われるようなPrioritized Experience Replayを使用する実験を行う．
具体的には盤面上の空きマスの数を$n$とすると， priorityを$\frac{1}{n}$とする~(ただし$n=0$のときはprioriry$=0.5$とした)．
priorityは$1$回学習に使用されるたびに， $0.5$倍に更新する．

図~\ref{fig:mid2048_alphazero}にこれらの手法の実験結果を示す．
実験では$10$ Residual Blockを， Stochastic MuZeroと同様に予測誤差をpriorityとするPrioritized Experience Replayを用いて訓練した手法が最も高得点を獲得した．
空きマスが少ない盤面データを優先的に学習させる手法~(図中のempty tiles replay)~は， 期待に反して他の手法と比べて効率的に学習が進むことはなかった．
全体として， 大きなサイズのニューラルネットワーク
またエージェントの得点は， 学習序盤に急激に伸びた後は緩やかに上昇することが見て取れる．
\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth{}]{figures/alphazero_4x3.pdf}
    \caption{$4\times3$盤面の2048-alphazero}
    \label{fig:mid2048_alphazero}
\end{figure}  

よってここまでの実験から2048の強化学習手法には以下の要素が重要だと考えられる．

\ref{sec:solving_impl}節で述べるように， $4\times3$盤面の2048の完全解析結果のデータベースは非常に大きく， データの参照などに現状時間を要する．
そのため本稿では，\ref{subsubsec:db_analize}節で$3\times3$盤面の2048で行ったような一手一手の細かい分析を行うことができなかった．
これは簡潔データ構造を用いることで改善することができると考えられる．
データベースの簡潔データ構造化， およびそれを用いた$4\times3$盤面の2048における強化学習手法の詳細な分析は製本版までの課題としたい．

\subsection{$4\times4$盤面の2048と2048-AlphaZero}
これまでのミニゲームでの実験を通して得た知見を踏まえて， オリジナルの$4\times4$盤面の2048で実験を行う．
ただし現状十分な実験を行えていないため， 製本版の提出時に実験結果を記載する．